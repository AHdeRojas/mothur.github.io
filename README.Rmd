
We need to get the mediawiki files from the wiki in mediawiki markup format

```{R}
library(tidyverse)
library(xml2)
library(rvest)

## Read page 1 of Special:AllPages
page1_url <- "https://mothur.org/wiki/Special:AllPages"
page1 <- xml2::read_html(page1_url)
page1_pages <- page1 %>% html_nodes("ul.mw-allpages-chunk") %>% html_nodes("li>:not(.mw-redirect)") %>% html_text()

## Read page 2 of Special:AllPages
page2_url <- page1 %>% html_nodes("a") %>% html_text() %>% str_subset(., "Next") %>% head(n=1) %>% str_replace(., "Next page \\((.*)\\)", "\\1") %>% paste0("https://mothur.org/w/index.php?title=Special:AllPages&from=", .)
page2 <- read_html(page2_url)
page2_pages <-  page2 %>% html_nodes("ul.mw-allpages-chunk") %>% html_nodes("li>:not(.mw-redirect)") %>% html_text()

## Confirm that there's no page 3 of Special:AllPages
page2 %>% html_nodes("a") %>% html_text() %>% str_detect(., "Next") %>% sum(.) == 0


## Page names, inserting an underscore for all spaces
pages <- c(page1_pages, page2_pages) %>% str_replace_all(., " ", "_") %>% str_replace_all(., "/", "%2F")


## Function to download raw mediawiki markup version of each page
download <- function(x){

	source <- paste0('https://www.mothur.org/w/index.php?title=', x, '&action=raw')
	destination <- paste0("_mw_files/", x, ".mw") %>%
			str_replace(., ":.*", ".mw") #remove : and following from filenames
	download.file(source, destination, quiet=TRUE)

}

dir.create("_mw_files", showWarnings=FALSE)
pages %>% map(., download)
```

There are a fair number of spam pages about viagra, gold, dresses, etc. that we'd like to remove

```{bash}
mkdir -p _helper_files
ls _mw_files/*mw > _helper_files/keep_files.txt

# manually edit file to remove spam pages

chmod -w _helper_files/keep_files.txt
```

Let's take those mw files that we downloaded, remove the spam, and convert them to markdown format.

```{R}
convert_mw_to_md <- function(mediawiki_file){

	#use pandoc to convert mediawiki markup to markdown
	markdown_file <- str_replace(mediawiki_file, "_mw_files/(.*).mw", "_wiki/\\1.md")
	system(paste0("pandoc -f mediawiki -t markdown ", mediawiki_file, " -o ", markdown_file))

	md <- scan(markdown_file, what=character(), quiet=TRUE, sep="\n", blank.lines.skip=FALSE)

	# convert second level headers denoted with underscores to pound symbols
	hyphen_lines <- str_which(md, "^---")
	if(length(hyphen_lines) > 0){
		md[hyphen_lines-1] <- paste0("## ", md[hyphen_lines-1])
		md <- md[-hyphen_lines]
	}

	# convert inline code into code blocks
	md <- str_replace(md, "[\\\\]+$", "")
	md <- str_replace(md, "`(.*)`$", "    \\1")

	#formatting for math
	md <- str_replace_all(md, "\\{\\{", "{\ {")
	md <- str_replace_all(md, "\\$", "$$")

	# get rid of weird wikilink tags
 	md <- str_replace_all(md, ' "wikilink"', '')

	# add yaml material
	title <- str_replace(markdown_file, "_wiki/(.*).md", "\\1") %>% str_replace_all(., "_", " ") %>% str_replace_all(., ":.*", "")

	if(any(str_detect(md, "Category:Commands"))){

		md <- c(
							"---",
							paste0("title: '", title, "'"),
							paste0("tags: 'commands'"),
							"---",
							md
						)

		md <- str_replace(md, "\\[Category:Commands\\]\\(Category:Commands\\)", "")

	} else {

		md <- c(
							"---",
							paste0("title: '", title, "'"),
							"---",
							md
						)
	}
	# remove the manually placed TOC
	md <- md[!str_detect(md, "\\\\_\\\\_TOC\\\\_\\\\_")]

	# remove tags for section headers from the mw files
 	md <- str_replace(md, " \\{\\#.*\\}", "")
	write(md, file=markdown_file)

	# if a page has a self-link, make it bold instead
	stub <- str_replace_all(markdown_file, "_wiki/(.*).md", "\\1") %>% tolower()
	bold_stub <- paste0("**", stub, "**")
	link_stub <- paste0("\\[", stub, "\\]\\(", stub, "\\)")
	md <- str_replace_all(md, link_stub, bold_stub)
	md <- str_replace_all(md, paste0(" ", stub, " "), paste0(" ", bold_stub, " "))

	# make everything https
	md <- str_replace_all(md, "http://", "https://")

	# replace Media links with links to S3 mothur bucket
	md <- gsub(md, pattern="Media:(.*?)\\)", replacement="https://mothur.s3.us-east-2.amazonaws.com/wiki/\\L\\1)", perl=TRUE, ignore.case=TRUE)

	# fix naked links to wiki
	naked_wiki_links <- any(str_detect(md, "<(http.*?/wiki/.*?)>"))
	md <- str_replace_all(md, "<http.*?/wiki/(.*?)>", "[\\1](\\1)")
	md <- str_replace_all(md, "<(http.*?)>", "[\\1](\\1)")

	# remove hard links to wiki
	md <- str_replace_all(md, "https://.{0,4}mothur.org/wiki/", "")

	# fix links to old forum
	md <- str_replace_all(md, "https://.{0,4}mothur.org/forum/", "https://forum.mothur.org/")

	# fix links to old version of blog
	md <- str_replace_all(md, "https://blog.mothur.org/(....)/../../", "/blog/\\1/")

	# remove stray : characters
	md <- str_replace_all(md, "  : ", "    ")

	# fix images
	md <- gsub(md, pattern="(!\\[.*?\\])\\(([^ ]*).*\\)", replacement="\\1(https://mothur.s3.us-east-2.amazonaws.com/wiki/\\L\\2)", perl=TRUE)
	md <- str_replace_all(md, "\\)\\{.*?\\}", ")")
	md <- str_replace(md, "^ +!", "!")

	# fix numbered lists
	md <- str_replace(md, "^(\\d*)\\.", "\n\\1\\\\.")


	write(md, file=markdown_file)

	naked_wiki_links
}

dir.create("_wiki", showWarnings=FALSE)
unlink("_wiki/*")

conversion <- read_tsv("_helper_files/keep_files.txt", col_name=c("file")) %>%
	mutate(convert = map_lgl(file, convert_mw_to_md))

unlink("_wiki/Main_Page.md")
```

## Additional files to scrap...
* Software_development
* mothur_newsletter


## Manual fixing
* Find and replace < and > characters with \gt and \lt in mathjax code
* Find and replace % with \% in mathjax code
* Check for stray `\$$` that are meant to indicate dollars
* Tables need help
* Figures will need to be manually resized
* get rid of stray `\"` and `\'` characters and replace with """ and "'" in atom
* Add calculators tag to calculators, remove calculators page and link
* Clear errors...
```
#https://github.com/gjtorikian/html-proofer
htmlproofer --assume-extension --only-4xx ./_site
```


## To do...
* blog
  - check on embedded images `!\[.*\]\(.*\)` in blog

* data
  - Set up links to large data files for projects

* general site
	* Customize header nav bar
	* Link to license on each page
	* Add announcement blurb
	* Fix space between toc and edits
	* fix stars/follows in the edit links
	* Google tracking code - see what I have in config.yml to see if it works
	* robots.txt page
